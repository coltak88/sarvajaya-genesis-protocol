# Prometheus Alert Rules for Sarvajaya Genesis Protocol AI System
groups:
  - name: ai_system_alerts
    rules:
      # AI System Health Alerts
      - alert: AISystemDown
        expr: up{job="sarvajaya-ai-backend"} == 0
        for: 1m
        labels:
          severity: critical
          service: ai-backend
          team: ai-ops
        annotations:
          summary: "AI system is down"
          description: "Sarvajaya Genesis Protocol AI backend has been down for more than 1 minute"

      - alert: AIHighResponseTime
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{job="sarvajaya-ai-backend"}[5m])) > 10
        for: 5m
        labels:
          severity: warning
          service: ai-backend
          team: ai-ops
        annotations:
          summary: "AI system high response time"
          description: "95th percentile response time is above 10 seconds for AI requests"

      - alert: AIHighErrorRate
        expr: rate(http_requests_total{job="sarvajaya-ai-backend",status=~"5.."}[5m]) / rate(http_requests_total{job="sarvajaya-ai-backend"}[5m]) > 0.05
        for: 5m
        labels:
          severity: warning
          service: ai-backend
          team: ai-ops
        annotations:
          summary: "AI system high error rate"
          description: "Error rate is above 5% for AI requests"

      - alert: AICriticalErrorRate
        expr: rate(http_requests_total{job="sarvajaya-ai-backend",status=~"5.."}[5m]) / rate(http_requests_total{job="sarvajaya-ai-backend"}[5m]) > 0.1
        for: 2m
        labels:
          severity: critical
          service: ai-backend
          team: ai-ops
        annotations:
          summary: "AI system critical error rate"
          description: "Error rate is above 10% for AI requests"

  - name: model_performance_alerts
    rules:
      # AI Model Performance Alerts
      - alert: ModelInferenceLatencyHigh
        expr: histogram_quantile(0.95, rate(ai_model_inference_duration_seconds_bucket[5m])) > 30
        for: 5m
        labels:
          severity: warning
          service: ai-models
          team: ai-ml
        annotations:
          summary: "Model inference latency is high"
          description: "95th percentile model inference time is above 30 seconds"

      - alert: ModelAccuracyDrop
        expr: ai_model_accuracy < 0.85
        for: 10m
        labels:
          severity: warning
          service: ai-models
          team: ai-ml
        annotations:
          summary: "Model accuracy dropped"
          description: "AI model accuracy has dropped below 85%"

      - alert: ModelAccuracyCritical
        expr: ai_model_accuracy < 0.75
        for: 5m
        labels:
          severity: critical
          service: ai-models
          team: ai-ml
        annotations:
          summary: "Model accuracy critically low"
          description: "AI model accuracy has dropped below 75%"

      - alert: ModelMemoryUsageHigh
        expr: ai_model_memory_usage_bytes / ai_model_memory_limit_bytes > 0.9
        for: 5m
        labels:
          severity: warning
          service: ai-models
          team: ai-ml
        annotations:
          summary: "Model memory usage high"
          description: "AI model memory usage is above 90%"

      - alert: ModelGPUMemoryHigh
        expr: ai_gpu_memory_used_bytes / ai_gpu_memory_total_bytes > 0.95
        for: 5m
        labels:
          severity: critical
          service: ai-models
          team: ai-ml
        annotations:
          summary: "GPU memory usage critically high"
          description: "GPU memory usage is above 95%"

  - name: vector_search_alerts
    rules:
      # Vector Search Alerts
      - alert: VectorSearchLatencyHigh
        expr: histogram_quantile(0.95, rate(vector_search_duration_seconds_bucket[5m])) > 5
        for: 5m
        labels:
          severity: warning
          service: vector-search
          team: ai-ops
        annotations:
          summary: "Vector search latency is high"
          description: "95th percentile vector search time is above 5 seconds"

      - alert: VectorSearchAccuracyDrop
        expr: vector_search_accuracy < 0.8
        for: 10m
        labels:
          severity: warning
          service: vector-search
          team: ai-ops
        annotations:
          summary: "Vector search accuracy dropped"
          description: "Vector search accuracy has dropped below 80%"

      - alert: VectorDatabaseDown
        expr: up{job="weaviate"} == 0
        for: 2m
        labels:
          severity: critical
          service: vector-db
          team: ai-ops
        annotations:
          summary: "Vector database is down"
          description: "Weaviate vector database has been down for more than 2 minutes"

  - name: content_generation_alerts
    rules:
      # Content Generation Alerts
      - alert: ContentGenerationFailureRateHigh
        expr: rate(content_generation_failures_total[5m]) / rate(content_generation_requests_total[5m]) > 0.1
        for: 5m
        labels:
          severity: warning
          service: content-generation
          team: ai-ops
        annotations:
          summary: "Content generation failure rate high"
          description: "Content generation failure rate is above 10%"

      - alert: ContentSafetyViolation
        expr: rate(content_safety_violations_total[5m]) > 0
        for: 1m
        labels:
          severity: critical
          service: content-generation
          team: ai-ops
        annotations:
          summary: "Content safety violation detected"
          description: "Content safety violation has been detected"

      - alert: ContentGenerationQueueBacklog
        expr: celery_queue_length{queue="content_generation"} > 100
        for: 10m
        labels:
          severity: warning
          service: content-generation
          team: ai-ops
        annotations:
          summary: "Content generation queue backlog"
          description: "Content generation queue has more than 100 pending tasks"

  - name: knowledge_base_alerts
    rules:
      # Knowledge Base Alerts
      - alert: KnowledgeBaseSyncFailure
        expr: rate(knowledge_base_sync_failures_total[5m]) > 0
        for: 5m
        labels:
          severity: warning
          service: knowledge-base
          team: ai-ops
        annotations:
          summary: "Knowledge base sync failure"
          description: "Knowledge base synchronization has failed"

      - alert: KnowledgeBaseIndexOutdated
        expr: time() - knowledge_base_last_index_timestamp > 3600
        for: 10m
        labels:
          severity: warning
          service: knowledge-base
          team: ai-ops
        annotations:
          summary: "Knowledge base index outdated"
          description: "Knowledge base index is more than 1 hour old"

  - name: system_resource_alerts
    rules:
      # System Resource Alerts
      - alert: HighCPUUsage
        expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 10m
        labels:
          severity: warning
          service: system
          team: ai-ops
        annotations:
          summary: "High CPU usage"
          description: "CPU usage is above 80%"

      - alert: CriticalCPUUsage
        expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 95
        for: 5m
        labels:
          severity: critical
          service: system
          team: ai-ops
        annotations:
          summary: "Critical CPU usage"
          description: "CPU usage is above 95%"

      - alert: HighMemoryUsage
        expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes * 100 > 85
        for: 10m
        labels:
          severity: warning
          service: system
          team: ai-ops
        annotations:
          summary: "High memory usage"
          description: "Memory usage is above 85%"

      - alert: CriticalMemoryUsage
        expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes * 100 > 95
        for: 5m
        labels:
          severity: critical
          service: system
          team: ai-ops
        annotations:
          summary: "Critical memory usage"
          description: "Memory usage is above 95%"

      - alert: HighDiskUsage
        expr: (node_filesystem_size_bytes{fstype!="tmpfs"} - node_filesystem_avail_bytes{fstype!="tmpfs"}) / node_filesystem_size_bytes{fstype!="tmpfs"} * 100 > 85
        for: 10m
        labels:
          severity: warning
          service: system
          team: ai-ops
        annotations:
          summary: "High disk usage"
          description: "Disk usage is above 85%"

  - name: database_alerts
    rules:
      # Database Alerts
      - alert: PostgreSQLDown
        expr: up{job="postgresql"} == 0
        for: 2m
        labels:
          severity: critical
          service: database
          team: ai-ops
        annotations:
          summary: "PostgreSQL is down"
          description: "PostgreSQL database has been down for more than 2 minutes"

      - alert: PostgreSQLHighConnections
        expr: pg_stat_activity_count > 80
        for: 5m
        labels:
          severity: warning
          service: database
          team: ai-ops
        annotations:
          summary: "PostgreSQL high connections"
          description: "PostgreSQL has more than 80 active connections"

      - alert: PostgreSQLSlowQueries
        expr: rate(pg_stat_statements_total_time_seconds[5m]) > 10
        for: 5m
        labels:
          severity: warning
          service: database
          team: ai-ops
        annotations:
          summary: "PostgreSQL slow queries"
          description: "PostgreSQL query execution time is above 10 seconds"

  - name: redis_alerts
    rules:
      # Redis Alerts
      - alert: RedisDown
        expr: up{job="redis"} == 0
        for: 2m
        labels:
          severity: critical
          service: cache
          team: ai-ops
        annotations:
          summary: "Redis is down"
          description: "Redis has been down for more than 2 minutes"

      - alert: RedisHighMemoryUsage
        expr: redis_memory_used_bytes / redis_memory_max_bytes > 0.9
        for: 5m
        labels:
          severity: warning
          service: cache
          team: ai-ops
        annotations:
          summary: "Redis high memory usage"
          description: "Redis memory usage is above 90%"

  - name: celery_alerts
    rules:
      # Celery Alerts
      - alert: CeleryWorkersDown
        expr: celery_workers_active < 1
        for: 2m
        labels:
          severity: critical
          service: celery
          team: ai-ops
        annotations:
          summary: "Celery workers are down"
          description: "No active Celery workers detected"

      - alert: CeleryHighQueueLength
        expr: celery_queue_length > 1000
        for: 10m
        labels:
          severity: warning
          service: celery
          team: ai-ops
        annotations:
          summary: "Celery high queue length"
          description: "Celery queue has more than 1000 pending tasks"

      - alert: CeleryHighFailureRate
        expr: rate(celery_task_failures_total[5m]) / rate(celery_tasks_total[5m]) > 0.05
        for: 5m
        labels:
          severity: warning
          service: celery
          team: ai-ops
        annotations:
          summary: "Celery high failure rate"
          description: "Celery task failure rate is above 5%"

  - name: api_usage_alerts
    rules:
      # API Usage Alerts
      - alert: APIRateLimitExceeded
        expr: rate(api_rate_limit_exceeded_total[5m]) > 0
        for: 1m
        labels:
          severity: warning
          service: api
          team: ai-ops
        annotations:
          summary: "API rate limit exceeded"
          description: "API rate limit has been exceeded"

      - alert: APIHighUsage
        expr: rate(api_requests_total[5m]) > 1000
        for: 5m
        labels:
          severity: info
          service: api
          team: ai-ops
        annotations:
          summary: "API high usage"
          description: "API is receiving more than 1000 requests per second"

      - alert: SubscriptionExpirationWarning
        expr: subscription_days_until_expiration < 7
        for: 1h
        labels:
          severity: warning
          service: subscription
          team: business
        annotations:
          summary: "Subscription expiring soon"
          description: "Subscription will expire in less than 7 days"

  - name: security_alerts
    rules:
      # Security Alerts
      - alert: FailedLoginAttempts
        expr: rate(failed_login_attempts_total[5m]) > 10
        for: 5m
        labels:
          severity: warning
          service: security
          team: security
        annotations:
          summary: "Failed login attempts"
          description: "More than 10 failed login attempts per minute"

      - alert: SuspiciousAPIActivity
        expr: rate(suspicious_api_activity_total[5m]) > 0
        for: 1m
        labels:
          severity: critical
          service: security
          team: security
        annotations:
          summary: "Suspicious API activity detected"
          description: "Suspicious API activity has been detected"

      - alert: SSLCertificateExpiring
        expr: ssl_certificate_expiry_seconds < 604800
        for: 1h
        labels:
          severity: warning
          service: security
          team: security
        annotations:
          summary: "SSL certificate expiring soon"
          description: "SSL certificate will expire in less than 7 days"

      - alert: SSLCertificateExpired
        expr: ssl_certificate_expiry_seconds < 0
        for: 1m
        labels:
          severity: critical
          service: security
          team: security
        annotations:
          summary: "SSL certificate expired"
          description: "SSL certificate has expired"

  - name: mlflow_alerts
    rules:
      # MLflow Alerts
      - alert: MLflowDown
        expr: up{job="mlflow"} == 0
        for: 5m
        labels:
          severity: warning
          service: mlflow
          team: ai-ml
        annotations:
          summary: "MLflow is down"
          description: "MLflow tracking server has been down for more than 5 minutes"

      - alert: ModelTrainingFailure
        expr: rate(mlflow_training_failures_total[5m]) > 0
        for: 1m
        labels:
          severity: warning
          service: mlflow
          team: ai-ml
        annotations:
          summary: "Model training failure"
          description: "Model training has failed"

      - alert: ModelDeploymentFailure
        expr: rate(mlflow_deployment_failures_total[5m]) > 0
        for: 1m
        labels:
          severity: critical
          service: mlflow
          team: ai-ml
        annotations:
          summary: "Model deployment failure"
          description: "Model deployment has failed"